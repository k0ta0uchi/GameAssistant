# -*- coding: utf-8 -*-
# é«˜é€Ÿãƒ»é«˜ç²¾åº¦AIæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆGroké¢¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: åºƒåŸŸæ¤œç´¢ + Re-ranking + Selected Scrapingï¼‰
import requests
from dotenv import load_dotenv
import os
import time
import logging
import asyncio
import re
from playwright.async_api import async_playwright
import numpy as np
from datetime import datetime
import math
from sklearn.metrics.pairwise import cosine_similarity
from .memory import get_embedding_model
from .clients import get_gemini_client

load_dotenv()

# --- è¨­å®š ---
BRAVE_API_KEY = os.environ.get("BRAVE_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL")

def transform_query_to_keywords(query: str) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã™ã‚‹"""
    if not GEMINI_MODEL:
        return query

    prompt = f"""
    ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«å…¥åŠ›ã™ã‚‹ãŸã‚ã®æœ€é©ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘ã‚’æŠ½å‡ºã—ã¦ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    ä½™è¨ˆãªèª¬æ˜ã‚„æŒ¨æ‹¶ã¯ä¸è¦ã§ã™ã€‚

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
    """
    
    try:
        logging.info(f"Transforming query to keywords: '{query[:50]}...'")
        client = get_gemini_client()
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ã‚ã«è¨­å®š
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt
        )
        if response and response.text:
            keywords = response.text.strip()
            # ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€ãªã©ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒå«ã¾ã‚Œã‚‹å ´åˆã‚’é™¤å»
            keywords = re.sub(r'^(ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|Keywords)[:ï¼š\s]+', '', keywords, flags=re.I)
            logging.info(f"Transformation success: '{keywords}'")
            return keywords
        else:
            logging.warning("Gemini returned empty response for keywords transformation.")
            return query
    except Exception as e:
        logging.warning(f"Failed to transform query to keywords (using original): {e}")
        return query

class BraveSearchClient:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.search.brave.com/res/v1/web/search"

    def search(self, query, count=50):
        if not self.api_key:
            logging.warning("BRAVE_API_KEY is not set.")
            return []
        
        headers = {
            "Accept": "application/json",
            "X-Subscription-Token": self.api_key
        }
        params = {
            "q": query,
            "count": count
        }
        
        try:
            res = requests.get(self.base_url, headers=headers, params=params)
            res.raise_for_status()
            data = res.json()
            return data.get("web", {}).get("results", [])
        except Exception as e:
            logging.error(f"Brave Search API failed: {e}")
            return []

async def fetch_page_content(browser_context, url):
    """Playwrightã‚’ä½¿ã£ã¦ãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹"""
    page = await browser_context.new_page()
    try:
        # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ï¼ˆç”»åƒã€CSSã€ãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
        async def block_aggressively(route):
            if route.request.resource_type in ["image", "stylesheet", "font", "media"]:
                await route.abort()
            else:
                await route.continue_()
        
        await page.route("**/*", block_aggressively)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ã‚ã«è¨­å®š (10ç§’)
        await page.goto(url, timeout=10000, wait_until="domcontentloaded")
        
        # æœ¬æ–‡ã®å–å¾—ï¼ˆä¸»è¦ãªã‚¿ã‚°ã®ã¿ã‹ã‚‰æŠ½å‡ºã—ã¦ç²¾åº¦ã¨é€Ÿåº¦ã‚’ä¸Šã’ã‚‹ï¼‰
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å–å¾—ã—ã¦çµåˆã™ã‚‹
        content = await page.evaluate("""() => {
            const title = document.title;
            const main = document.querySelector('main') || document.querySelector('article') || document.body;
            
            // ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            const scriptTags = main.querySelectorAll('script, style, nav, footer, header, noscript, iframe, .ad, .ads, .social-share');
            scriptTags.forEach(s => s.remove());
            
            return `TITLE: ${title}\n\n${main.innerText}`;
        }""",
        )
        
        return content[:10000] # æ–‡å­—æ•°åˆ¶é™
        
    except Exception as e:
        logging.warning(f"Error loading {url}: {e}")
        return None
    finally:
        await page.close()

def calculate_freshness_score(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰é®®åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ (æ–°ã—ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢)"""
    if not date_str:
        return 0.0
    try:
        # Braveã®æ—¥ä»˜å½¢å¼ã«å¯¾å¿œ (ä¾‹: "2023-10-27T...")
        # å½¢å¼ãŒå¤šæ§˜ãªãŸã‚ã€ç°¡æ˜“çš„ãªãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        dt = None
        for fmt in ["%Y-%m-%d", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ"]:
            try:
                dt = datetime.strptime(date_str.split('T')[0], "%Y-%m-%d")
                break
            except ValueError:
                continue
        
        if dt:
            days_old = (datetime.now() - dt).days
            if days_old < 0: days_old = 0
            # æ¸›è¡°é–¢æ•°: 1å¹´(365æ—¥)ã§ç´„0.37å€ã«ãªã‚‹æŒ‡æ•°æ¸›è¡°
            return math.exp(-days_old / 365.0)
    except Exception:
        pass
    return 0.0

async def ai_search(query):
    logging.info(f"ğŸ” AI Web Search (Grok-style) Started: {query}")
    
    # 0. ã‚¯ã‚¨ãƒªå¤‰æ› (Natural Language -> Search Keywords)
    search_keywords = transform_query_to_keywords(query)
    if not search_keywords or len(search_keywords.strip()) == 0:
        search_keywords = query
        logging.info("Using original query as search keywords.")

    # 1. åºƒç¯„å›²æ¤œç´¢ (Brave Search API) using Keywords
    logging.info(f"Brave Search API Request: '{search_keywords}'")
    brave_client = BraveSearchClient(BRAVE_API_KEY)
    raw_results = brave_client.search(search_keywords, count=50)
    
    if not raw_results:
        logging.warning("Brave Search returned 0 results. Search aborted.")
        return []

    logging.info(f"Brave Search returned {len(raw_results)} results.")

    # 2. Re-ranking (Embedding + Cosine Similarity) using ORIGINAL Query
    logging.info("Calculating embeddings for re-ranking...")
    try:
        embedding_model = get_embedding_model()
        
        # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ– (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ±²ã‚€ãŸã‚å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨)
        query_vec = embedding_model.encode(query, show_progress_bar=False)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼‰ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        # title + description ã‚’é€£çµ
        docs_text = [f"{item.get('title', '')} {item.get('description', '')}" for item in raw_results]
        docs_vecs = embedding_model.encode(docs_text, show_progress_bar=False) # ãƒãƒƒãƒå‡¦ç†
        
        # é¡ä¼¼åº¦è¨ˆç®—        # reshape(1, -1) ã§2æ¬¡å…ƒé…åˆ—ã«ã™ã‚‹
        similarities = cosine_similarity(query_vec.reshape(1, -1), docs_vecs)[0]
        
        scored_results = []
        for i, item in enumerate(raw_results):
            relevance_score = similarities[i]
            
            # é®®åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®— (ageãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹å ´åˆ)
            freshness_score = 0.0
            if 'age' in item:
                freshness_score = calculate_freshness_score(item['age'])
            
            # æœ€çµ‚ã‚¹ã‚³ã‚¢: é–¢é€£åº¦é‡è¦–ã ãŒã€é®®åº¦ã‚‚åŠ å‘³
            final_score = relevance_score * 0.8 + freshness_score * 0.2
            
            scored_results.append({
                "item": item,
                "score": final_score,
                "relevance": relevance_score
            })
        
        # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½5ä»¶ã‚’æŠ½å‡º
        top_results = sorted(scored_results, key=lambda x: x['score'], reverse=True)[:5]
        
        logging.info(f"Top 5 results selected (Score range: {top_results[0]['score']:.3f} - {top_results[-1]['score']:.3f})")
        for res in top_results:
            logging.info(f"- [{res['score']:.3f}] {res['item'].get('title')} ({res['item'].get('url')})")
    except Exception as e:
        logging.error(f"Re-ranking process failed: {e}", exc_info=True)
        # å¤±æ•—ã—ãŸå ´åˆã¯Braveã®æ¤œç´¢çµæœã®ä¸Šä½5ä»¶ã‚’ãã®ã¾ã¾ä½¿ã†
        top_results = [{"item": item, "score": 0.0} for item in raw_results[:5]]
        logging.info("Falling back to top 5 results from Brave.")

    # 3. Selected Scraping (ä¸Šä½è¨˜äº‹ã®æœ¬æ–‡å–å¾—)
    logging.info(f"Starting scraping for {len(top_results)} URLs...")
    urls = [res['item']['url'] for res in top_results]
    
    scraped_contents = []
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            )
            
            tasks = [fetch_page_content(context, url) for url in urls]
            contents = await asyncio.gather(*tasks)
            
            for i, content in enumerate(contents):
                item = top_results[i]['item']
                if content and len(content.strip()) > 100:
                    logging.info(f"Successfully scraped content from: {item.get('url')} ({len(content)} chars)")
                    scraped_contents.append(f"### Source: {item.get('title')}\nURL: {item.get('url')}\n\n{content}\n")
                else:
                    logging.warning(f"Scraping yielded poor/no content for: {item.get('url')}. Using snippet instead.")
                    fallback = f"### Source: {item.get('title')}\nURL: {item.get('url')}\n(Note: Content fetch failed, using snippet)\n{item.get('description')}\n"
                    scraped_contents.append(fallback)
            
            await browser.close()
    except Exception as e:
        logging.error(f"Fatal error during scraping process: {e}", exc_info=True)
        # å…¨ä½“çš„ã«å¤±æ•—ã—ãŸå ´åˆã¯å…¨ä»¶ã‚¹ãƒ‹ãƒšãƒƒãƒˆã§è¿”ã™
        for res in top_results:
            item = res['item']
            scraped_contents.append(f"### Source: {item.get('title')}\nURL: {item.get('url')}\n(Snippet only due to error)\n{item.get('description')}\n")

    logging.info(f"AI Search completed. Returning {len(scraped_contents)} sources.")
    return scraped_contents